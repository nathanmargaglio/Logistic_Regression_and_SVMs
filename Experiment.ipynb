{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "from random import randrange\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\" \n",
    "     Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "    \"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "\n",
    "    n_feature = mat.get(\"train1\").shape[1]\n",
    "    n_sample = 0\n",
    "    for i in range(10):\n",
    "        n_sample = n_sample + mat.get(\"train\" + str(i)).shape[0]\n",
    "    n_validation = 1000\n",
    "    n_train = n_sample - 10 * n_validation\n",
    "\n",
    "    # Construct validation data\n",
    "    validation_data = np.zeros((10 * n_validation, n_feature))\n",
    "    for i in range(10):\n",
    "        validation_data[i * n_validation:(i + 1) * n_validation, :] = mat.get(\"train\" + str(i))[0:n_validation, :]\n",
    "\n",
    "    # Construct validation label\n",
    "    validation_label = np.ones((10 * n_validation, 1))\n",
    "    for i in range(10):\n",
    "        validation_label[i * n_validation:(i + 1) * n_validation, :] = i * np.ones((n_validation, 1))\n",
    "\n",
    "    # Construct training data and label\n",
    "    train_data = np.zeros((n_train, n_feature))\n",
    "    train_label = np.zeros((n_train, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"train\" + str(i)).shape[0]\n",
    "        train_data[temp:temp + size_i - n_validation, :] = mat.get(\"train\" + str(i))[n_validation:size_i, :]\n",
    "        train_label[temp:temp + size_i - n_validation, :] = i * np.ones((size_i - n_validation, 1))\n",
    "        temp = temp + size_i - n_validation\n",
    "\n",
    "    # Construct test data and label\n",
    "    n_test = 0\n",
    "    for i in range(10):\n",
    "        n_test = n_test + mat.get(\"test\" + str(i)).shape[0]\n",
    "    test_data = np.zeros((n_test, n_feature))\n",
    "    test_label = np.zeros((n_test, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"test\" + str(i)).shape[0]\n",
    "        test_data[temp:temp + size_i, :] = mat.get(\"test\" + str(i))\n",
    "        test_label[temp:temp + size_i, :] = i * np.ones((size_i, 1))\n",
    "        temp = temp + size_i\n",
    "\n",
    "    # Delete features which don't provide any useful information for classifiers\n",
    "    sigma = np.std(train_data, axis=0)\n",
    "    index = np.array([])\n",
    "    for i in range(n_feature):\n",
    "        if (sigma[i] > 0.001):\n",
    "            index = np.append(index, [i])\n",
    "    train_data = train_data[:, index.astype(int)]\n",
    "    validation_data = validation_data[:, index.astype(int)]\n",
    "    test_data = test_data[:, index.astype(int)]\n",
    "\n",
    "    # Scale data to 0 and 1\n",
    "    train_data /= 255.0\n",
    "    validation_data /= 255.0\n",
    "    test_data /= 255.0\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def blrObjFunction(initialWeights, *args):\n",
    "    \"\"\"\n",
    "    blrObjFunction computes 2-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights: the weight vector (w_k) of size (D + 1) x 1\n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector (y_k) of size N x 1 where each entry can be either 0 or 1 representing the label of corresponding feature vector\n",
    "\n",
    "    Output:\n",
    "        error: the scalar value of error function of 2-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 1 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    train_data, labeli = args\n",
    "\n",
    "    n_data = train_data.shape[0]\n",
    "    n_features = train_data.shape[1]\n",
    "\n",
    "    intercept = np.ones((n_data, 1))\n",
    "    x = np.hstack((intercept, train_data))\n",
    "    h = sigmoid(np.dot(x, initialWeights)).reshape(n_data, 1)\n",
    "    y = labeli\n",
    "\n",
    "    error = -sum(y * np.log(h) + (1 - y) * np.log(1 - h)) / n_data\n",
    "    error_grad = np.dot(x.T, (h - y)).reshape(n_features + 1, ) / n_data\n",
    "\n",
    "    return error, error_grad\n",
    "\n",
    "\n",
    "def blrPredict(W, data):\n",
    "    \"\"\"\n",
    "     blrObjFunction predicts the label of data given the data and parameter W\n",
    "     of Logistic Regression\n",
    "\n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "\n",
    "     Output:\n",
    "         label: vector of size N x 1 representing the predicted label of\n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n_data = data.shape[0]\n",
    "\n",
    "    intercept = np.ones((n_data, 1))\n",
    "    x = np.hstack((intercept, data))\n",
    "\n",
    "    label = sigmoid(np.dot(x, W))\n",
    "\n",
    "    return np.argmax(label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for Logistic Regression\n",
    "\"\"\"\n",
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "# number of classes\n",
    "n_class = 10\n",
    "\n",
    "# number of training samples\n",
    "n_train = train_data.shape[0]\n",
    "\n",
    "# number of features\n",
    "n_feature = train_data.shape[1]\n",
    "\n",
    "Y = np.zeros((n_train, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
    "    \n",
    "W = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights = np.zeros((n_feature + 1, 1))\n",
    "opts = {'maxiter': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with Gradient Descent\n",
    "W = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights = np.zeros((n_feature + 1, 1))\n",
    "opts = {'maxiter': 100}\n",
    "for i in range(n_class):\n",
    "    print(i)\n",
    "    labeli = Y[:, i].reshape(n_train, 1)\n",
    "    args = (train_data, labeli)\n",
    "    nn_params = minimize(blrObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)\n",
    "    W[:, i] = nn_params.x.reshape((n_feature + 1,))\n",
    "    \n",
    "# notice the use of flatten()\n",
    "\n",
    "predicted_label = blrPredict(W, train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label.flatten()))) + '%')\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label = blrPredict(W, validation_data)\n",
    "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label.flatten()))) + '%')\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label = blrPredict(W, test_data)\n",
    "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label.flatten()))) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for Support Vector Machine\n",
    "\"\"\"\n",
    "\n",
    "print('\\n\\n--------------SVM-------------------\\n\\n')\n",
    "\n",
    "print(\"Linear\")\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(train_data, train_label)\n",
    "\n",
    "predicted_label = clf.predict(train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label.flatten()))) + '%')\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label = clf.predict(validation_data)\n",
    "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label.flatten()))) + '%')\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label = clf.predict(test_data)\n",
    "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label.flatten()))) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Radial; Gamma: 1\")\n",
    "clf = SVC(kernel='rbf', gamma=1)\n",
    "clf.fit(train_data, train_label)\n",
    "\n",
    "predicted_label = clf.predict(train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label.flatten()))) + '%')\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label = clf.predict(validation_data)\n",
    "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label.flatten()))) + '%')\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label = clf.predict(test_data)\n",
    "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label.flatten()))) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Radial; Gamma: Default\")\n",
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(train_data, train_label)\n",
    "\n",
    "predicted_label = clf.predict(train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label.flatten()))) + '%')\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label = clf.predict(validation_data)\n",
    "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label.flatten()))) + '%')\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label = clf.predict(test_data)\n",
    "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label.flatten()))) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {}\n",
    "\n",
    "for C in [100]:#[1] + list(np.arange(10, 110, 10)):\n",
    "    print(\"Radial; C:\", C)\n",
    "    clf = SVC(C=C)\n",
    "    clf.fit(train_data, train_label)\n",
    "    \n",
    "    preds[C] = {}\n",
    "\n",
    "    predicted_label = clf.predict(train_data)\n",
    "    print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label.flatten()))) + '%')\n",
    "    preds[C]['train'] = 100 * np.mean((predicted_label == train_label.flatten()))\n",
    "\n",
    "    # Find the accuracy on Validation Dataset\n",
    "    predicted_label = clf.predict(validation_data)\n",
    "    print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label.flatten()))) + '%')\n",
    "    preds[C]['validation'] = 100 * np.mean((predicted_label == validation_label.flatten()))\n",
    "\n",
    "    # Find the accuracy on Testing Dataset\n",
    "    predicted_label = clf.predict(test_data)\n",
    "    print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label.flatten()))) + '%')\n",
    "    preds[C]['testing'] = 100 * np.mean((predicted_label == test_label.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {\n",
    "    'logistic': [\n",
    "        92.724,\n",
    "        91.46,\n",
    "        92.0\n",
    "    ],\n",
    "    'svm_linear': [\n",
    "        97.286,\n",
    "        93.64,\n",
    "        93.78\n",
    "    ],\n",
    "    'svm_gamma_1': [\n",
    "        100.0,\n",
    "        15.48,\n",
    "        17.14\n",
    "    ],\n",
    "    'svm_gamma_default': [\n",
    "        94.294,\n",
    "        94.02,\n",
    "        94.42\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_log = \"\"\"\n",
    "08-11 15:20 root         INFO     \n",
    " Training set Accuracy:94.294%\n",
    "08-11 15:22 root         INFO     \n",
    " Validation set Accuracy:94.02000000000001%\n",
    "08-11 15:25 root         INFO     \n",
    " Testing set Accuracy:94.42%\n",
    "08-11 15:35 root         INFO     \n",
    " Training set Accuracy:97.13199999999999%\n",
    "08-11 15:37 root         INFO     \n",
    " Validation set Accuracy:96.17999999999999%\n",
    "08-11 15:38 root         INFO     \n",
    " Testing set Accuracy:96.1%\n",
    "08-11 15:48 root         INFO     \n",
    " Training set Accuracy:97.952%\n",
    "08-11 15:49 root         INFO     \n",
    " Validation set Accuracy:96.89999999999999%\n",
    "08-11 15:50 root         INFO     \n",
    " Testing set Accuracy:96.67%\n",
    "08-11 15:59 root         INFO     \n",
    " Training set Accuracy:98.372%\n",
    "08-11 16:00 root         INFO     \n",
    " Validation set Accuracy:97.1%\n",
    "08-11 16:02 root         INFO     \n",
    " Testing set Accuracy:97.04%\n",
    "08-11 16:11 root         INFO     \n",
    " Training set Accuracy:98.706%\n",
    "08-11 16:12 root         INFO     \n",
    " Validation set Accuracy:97.23%\n",
    "08-11 16:13 root         INFO     \n",
    " Testing set Accuracy:97.19%\n",
    "08-11 16:22 root         INFO     \n",
    " Training set Accuracy:99.002%\n",
    "08-11 16:23 root         INFO     \n",
    " Validation set Accuracy:97.31%\n",
    "08-11 16:24 root         INFO     \n",
    " Testing set Accuracy:97.19%\n",
    "08-11 16:33 root         INFO     \n",
    " Training set Accuracy:99.196%\n",
    "08-11 16:34 root         INFO     \n",
    " Validation set Accuracy:97.38%\n",
    "08-11 16:35 root         INFO     \n",
    " Testing set Accuracy:97.16%\n",
    "08-11 16:44 root         INFO     \n",
    " Training set Accuracy:99.33999999999999%\n",
    "08-11 16:45 root         INFO     \n",
    " Validation set Accuracy:97.36%\n",
    "08-11 16:46 root         INFO     \n",
    " Testing set Accuracy:97.26%\n",
    "08-11 16:55 root         INFO     \n",
    " Training set Accuracy:99.438%\n",
    "08-11 16:56 root         INFO     \n",
    " Validation set Accuracy:97.39%\n",
    "08-11 16:57 root         INFO     \n",
    " Testing set Accuracy:97.33000000000001%\n",
    "08-11 17:06 root         INFO     \n",
    " Training set Accuracy:99.542%\n",
    "08-11 17:07 root         INFO     \n",
    " Validation set Accuracy:97.36%\n",
    "08-11 17:08 root         INFO     \n",
    " Testing set Accuracy:97.34%\n",
    "08-11 17:17 root         INFO     \n",
    " Training set Accuracy:99.612%\n",
    "08-11 17:18 root         INFO     \n",
    " Validation set Accuracy:97.41%\n",
    "08-11 17:19 root         INFO     \n",
    " Testing set Accuracy:97.39999999999999%\n",
    "08-11 17:19 root         INFO     \n",
    " Training set Accuracy:99.612%\n",
    "08-11 17:19 root         INFO     \n",
    " Validation set Accuracy:97.41%\n",
    "08-11 17:19 root         INFO     \n",
    " Testing set Accuracy:9.8%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [1] + list(np.arange(10, 110, 10))\n",
    "accs = []\n",
    "\n",
    "for n, row in enumerate(raw_log.split('\\n')):\n",
    "    if not n % 2:\n",
    "        _row = row.split(':')\n",
    "        if _row[0]:\n",
    "            _row = float(_row[1].split('%')[0])\n",
    "            accs.append(_row)\n",
    "\n",
    "training_accs = []\n",
    "validation_accs = []\n",
    "testing_accs = []\n",
    "for n, a in enumerate(accs):\n",
    "    if n % 3 == 0:\n",
    "        training_accs.append(a)\n",
    "    if n % 3 == 1:\n",
    "        validation_accs.append(a)\n",
    "    if n % 3 == 2:\n",
    "        testing_accs.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [20,10]\n",
    "plt.plot(Cs, training_accs[:-1])\n",
    "plt.xticks(Cs)\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"C Value\")\n",
    "plt.suptitle(\"Accuracy of Prediction using SVM with varrying C Parameter\")\n",
    "plt.savefig('fig1.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "# number of classes\n",
    "n_class = 10\n",
    "\n",
    "# number of training samples\n",
    "n_train = train_data.shape[0]\n",
    "\n",
    "# number of features\n",
    "n_feature = train_data.shape[1]\n",
    "\n",
    "Y = np.zeros((n_train, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
    "    \n",
    "W = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights = np.zeros((n_feature + 1, 1))\n",
    "opts = {'maxiter': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlrObjFunction(params, *args):\n",
    "    \"\"\"\n",
    "    mlrObjFunction computes multi-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights: the weight vector of size (D + 1) x 1\n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector of size N x 1 where each entry can be either 0 or 1\n",
    "                representing the label of corresponding feature vector\n",
    "\n",
    "    Output:\n",
    "        error: the scalar value of error function of multi-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 10 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    train_data, Y = args\n",
    "\n",
    "    n_data = train_data.shape[0]\n",
    "    n_features = train_data.shape[1]\n",
    "\n",
    "    intercept = np.ones((n_data, 1))\n",
    "    x = np.hstack((intercept, train_data))\n",
    "    h = sigmoid(np.dot(x, initialWeights)).reshape(n_data, 1)\n",
    "    y = Y\n",
    "    \n",
    "    error = -sum(sum(y * np.log(h))) / n_data\n",
    "    error_grad = np.dot(x.T, (h - y)).reshape((n_features + 1)*10, ) / n_data\n",
    "\n",
    "    return error, error_grad\n",
    "\n",
    "\n",
    "def mlrPredict(W, data):\n",
    "    \"\"\"\n",
    "     mlrObjFunction predicts the label of data given the data and parameter W\n",
    "     of Logistic Regression\n",
    "\n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "\n",
    "     Output:\n",
    "         label: vector of size N x 1 representing the predicted label of\n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "    n_data = data.shape[0]\n",
    "\n",
    "    intercept = np.ones((n_data, 1))\n",
    "    x = np.hstack((intercept, data))\n",
    "\n",
    "    label = sigmoid(np.dot(x, W))\n",
    "\n",
    "    return np.argmax(label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training set Accuracy:9.846%\n",
      "\n",
      " Validation set Accuracy:10.0%\n",
      "\n",
      " Testing set Accuracy:9.8%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script for Extra Credit Part\n",
    "\"\"\"\n",
    "# FOR EXTRA CREDIT ONLY\n",
    "W_b = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights_b = np.zeros((n_feature + 1, n_class))\n",
    "opts_b = {'maxiter': 100}\n",
    "\n",
    "rindex = np.random.choice(np.arange(0, len(train_data)), size=1000, replace=False)\n",
    "args_b = (train_data[rindex], Y[rindex])\n",
    "nn_params = minimize(mlrObjFunction, initialWeights_b, jac=True, args=args_b, method='CG', options=opts_b)\n",
    "W_b = nn_params.x.reshape((n_feature + 1, n_class))\n",
    "\n",
    "# Find the accuracy on Training Dataset\n",
    "predicted_label_b = mlrPredict(W_b, train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label_b == train_label.flatten()).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label_b = mlrPredict(W_b, validation_data)\n",
    "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label_b == validation_label.flatten()).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label_b = mlrPredict(W_b, test_data)\n",
    "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label_b == test_label.flatten()).astype(float))) + '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = train_data.shape[0]\n",
    "n_features = train_data.shape[1]\n",
    "\n",
    "intercept = np.ones((n_data, 1))\n",
    "x = np.hstack((intercept, train_data))\n",
    "h = sigmoid(np.dot(x, initialWeights)).reshape(n_data, 1)\n",
    "y = Y\n",
    "\n",
    "error = -sum(sum(y * np.log(h))) / n_data\n",
    "error_grad = np.dot(x.T, (h - y)).reshape((n_features + 1)*10, ) / n_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5],\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       ...,\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       [0.5]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c6544ed30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.append(train_data[rindex][5], [0]*14).reshape((27,27)))\n",
    "print(Y[rindex][5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
